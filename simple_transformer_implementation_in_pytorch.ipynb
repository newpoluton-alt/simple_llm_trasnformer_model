{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Simple Transformer Implementation in PyTorch\n",
    "\n",
    "Architecture of the transformer model:\n",
    "* Input Embedding with Positional Encoding\n",
    "* Multi-Head Attention\n",
    "* Multi-Layer Perceptron (MLP)\n",
    "* Transformer Block\n",
    "* Output Layer\n",
    "\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import required libraries\n",
   "id": "20e80ab5878b48b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:47.894062Z",
     "start_time": "2026-02-08T21:25:46.967633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Tuple, List\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import math\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:47.916914Z",
     "start_time": "2026-02-08T21:25:47.907849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ],
   "id": "95018eb7b1c85cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Preparation",
   "id": "a4d7426c1fad5dd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download the tiny Shakespeare dataset",
   "id": "d7971e470bd0d2c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:47.936581Z",
     "start_time": "2026-02-08T21:25:47.926667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "FILE_NAME = \"input.txt\"\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "FULL_PATH = os.path.join(DATA_DIR, FILE_NAME)\n",
    "\n",
    "# Download the dataset if it doesn't exist\n",
    "if not os.path.exists(FULL_PATH):\n",
    "    urllib.request.urlretrieve(URL, FULL_PATH)\n",
    "\n",
    "print(\"Dataset downloaded to:\", FULL_PATH)\n",
    "\n",
    "with open(FULL_PATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print(\"Dataset length (characters):\", len(text))"
   ],
   "id": "3dc0d33e9c3708a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: data/input.txt\n",
      "Dataset length (characters): 1115394\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenization",
   "id": "ca5b087856f28714"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:47.952684Z",
     "start_time": "2026-02-08T21:25:47.947029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "VOCAB_SIZE = int(math.sqrt(len(text)))\n",
    "TOKENIZER_SAVE_PATH = os.path.join(DATA_DIR, \"bpe_tokenizer.json\")\n",
    "\n",
    "def train_bpe_tokenizer(text: str, vocab_size:int, save_path: str) -> Tokenizer:\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens = [\"[UNK]\", \"[PAD]\", \"[BOS]\", \"[EOS]\"],\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "    tokenizer.save(save_path)\n",
    "\n",
    "    print(\"Tokenizer trained and saved to:\", save_path)\n",
    "    return tokenizer"
   ],
   "id": "70544c5f47030953",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:48.097518Z",
     "start_time": "2026-02-08T21:25:47.953718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = train_bpe_tokenizer(text, VOCAB_SIZE, TOKENIZER_SAVE_PATH)\n",
    "\n"
   ],
   "id": "122386bb5297b6ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained and saved to: data/bpe_tokenizer.json\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create PyTorch Dataset and DataLoader",
   "id": "26899d2c89b85ee8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:48.112244Z",
     "start_time": "2026-02-08T21:25:48.098805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: Tokenizer, seq_length: int = 128, train: bool = True, train_split: float = 0.8):\n",
    "        self.text = text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        self.train = train\n",
    "        self.train_split = train_split\n",
    "\n",
    "        # Encode the entire text\n",
    "        encoded_text = self.tokenizer.encode(self.text)\n",
    "        self.tokens = torch.tensor(encoded_text.ids, dtype=torch.long)\n",
    "\n",
    "        # Split into train and validation sets\n",
    "        split_idx = int(len(self.tokens) * self.train_split)\n",
    "        if self.train:\n",
    "            self.tokens = self.tokens[:split_idx]\n",
    "        else:\n",
    "            self.tokens = self.tokens[split_idx:]\n",
    "\n",
    "        print(\"Dataset initialized. Total tokens:\", len(self.tokens))\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.tokens) - self.seq_length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_length], self.tokens[idx+1:idx+self.seq_length+1]\n"
   ],
   "id": "e25734be5ea7fdf1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:48.541771Z",
     "start_time": "2026-02-08T21:25:48.113207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create train and validation datasets\n",
    "SEQ_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = CustomDataset(text, tokenizer, seq_length=SEQ_LENGTH, train=True)\n",
    "val_dataset = CustomDataset(text, tokenizer, seq_length=SEQ_LENGTH, train=False)\n",
    "\n",
    "# Create DataLoaders\n",
    "# For training, we shuffle the data to ensure the model sees different sequences each epoch\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# Note: For validation, we typically don't shuffle the data because we want to evaluate on the same sequence of data each epoch\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "id": "17dc599f44ad4f9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized. Total tokens: 303268\n",
      "Dataset initialized. Total tokens: 75817\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformer Model Implementation",
   "id": "abbccca56a57689b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Transformer Input Embedding with Sinusoidal Positional Encoding",
   "id": "63e618dd7e8261e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:48.556717Z",
     "start_time": "2026-02-08T21:25:48.542885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerInputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, max_seq_length: int, *args: Any, **kwargs: Any):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        pe = self._create_positional_encoding(max_seq_length, embed_dim)\n",
    "        self.register_buffer('positional_encoding', pe)\n",
    "\n",
    "\n",
    "    def _create_positional_encoding(self, max_seq_length: int, embed_dim: int) -> torch.Tensor:\n",
    "        pe = torch.zeros(max_seq_length, embed_dim) # Shape (max_seq_length, embed_dim)\n",
    "        position = torch.arange(0, max_seq_length).unsqueeze(1).float() # Shape (max_seq_length, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim)) # Shape (embed_dim/2,)\n",
    "        # For even elements where we get shape (max_seq_length, ceil(embed_dim/2)) after multiplication\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        # For odd elements where we get shape (max_seq_length, ceil(embed_dim/2)) after multiplication\n",
    "        pe[:,1::2] = torch.cos(position * div_term[:pe[:,1::2].shape[1]])\n",
    "\n",
    "        return pe.unsqueeze(0)  # Shape: (1, max_seq_length, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch, seq_len), dtype long\n",
    "        seq_length = x.size(1)\n",
    "        if seq_length > self.positional_encoding.size(1):\n",
    "            raise ValueError(f\"seq_length={seq_length} exceeds max_seq_length={self.positional_encoding.size(1)}\")\n",
    "\n",
    "        token_embeds = self.token_embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        pos_embeds = self.positional_encoding[:, :seq_length, :]  # (1, seq_len, embed_dim)\n",
    "        return token_embeds + pos_embeds"
   ],
   "id": "8c40f42d19d59bbd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multi-Head Attention",
   "id": "af86cbcba29fc2a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:48.564778Z",
     "start_time": "2026-02-08T21:25:48.557576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1, *args: Any, **kwargs: Any):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # embed_dim must be divisible by num_heads\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                attn_mask: torch.Tensor | None = None,\n",
    "                key_padding_mask: torch.Tensor | None = None,\n",
    "                is_causal: bool = False, return_attn =False\n",
    "                ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, embed_dim)\n",
    "            attn_mask: optional mask broadcastable to (batch, num_heads, seq_len, seq_len)\n",
    "                - bool mask: True means \"block this attention\"\n",
    "                - or float mask: additive (e.g., 0 for allow, -inf for block)\n",
    "            key_padding_mask: (batch, seq_len) bool, True for PAD positions to ignore\n",
    "            is_causal: if True, prevents attending to future tokens (GPT-style)\n",
    "            return_attn: if True, also returns attention weights (batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            out: (batch, seq_len, embed_dim)\n",
    "            (optional) attn_weights\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Linear projections to get Q, K, V: (batch, seq_len, embed_dim)\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # Reshape for multi-head: (batch, num_heads, seq_len, head_dim)\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention scores: (batch, num_heads, seq_len, seq_len)\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * scale\n",
    "\n",
    "        # value used as a negative infinity for masking\n",
    "        neg_inf = torch.finfo(attn_scores.dtype).min\n",
    "\n",
    "        # Causal mask (prevent attending to future tokens)\n",
    "        if is_causal:\n",
    "            causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device, dtype=torch.bool), diagonal=1) # (seq_len, seq_len)\n",
    "            attn_scores = attn_scores.masked_fill(causal_mask, neg_inf)\n",
    "\n",
    "        # Attention mask (e.g., for padding)\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_scores = attn_scores.masked_fill(attn_mask, neg_inf)\n",
    "            else:\n",
    "                attn_scores = attn_scores + attn_mask\n",
    "\n",
    "        # Key padding mask (mask out PAD tokens)\n",
    "        if key_padding_mask is not None:\n",
    "            # key padding mask: (batch, seq_len) -> (batch, 1, 1, seq_len) for broadcasting\n",
    "            key_padding_mask = key_padding_mask[:, None, None, :]\n",
    "            attn_scores = attn_scores.masked_fill(key_padding_mask, neg_inf)\n",
    "\n",
    "        # Attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        context = attn_weights @ v  # (batch, num_heads, seq_len, head_dim)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)  # (batch, seq_len, embed_dim\n",
    "\n",
    "        # Final linear projection\n",
    "        out = self.out_proj(context)  # (batch, seq_len, embed_dim)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        if return_attn:\n",
    "            return out, attn_weights\n",
    "\n",
    "        return out\n",
    "\n"
   ],
   "id": "2bde0a27218cb290",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multi-Layer Perceptron (MLP)",
   "id": "1abe7719b918a30b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:48.571092Z",
     "start_time": "2026-02-08T21:25:48.565574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim: int, hidden_dim: int, dropout: float = 0.1, activation: str = \"gelu\",*args: Any, **kwargs: Any):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        act = activation.lower()\n",
    "\n",
    "        switcher = {\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"silu\": nn.SiLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"leakyrelu\": nn.LeakyReLU(),\n",
    "        }\n",
    "\n",
    "        if act not in switcher:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = switcher[act]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x) # (batch, seq_len, embed_dim)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ],
   "id": "116c4cb5ed3d996c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Transformer block implementation",
   "id": "283c662b0407dade"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global configuration for the transformer model",
   "id": "43b7b9a53c67a2d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:48.576486Z",
     "start_time": "2026-02-08T21:25:48.571537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = VOCAB_SIZE\n",
    "    max_seq_length: int = SEQ_LENGTH\n",
    "    embed_dim: int = 256\n",
    "    num_heads: int = 8\n",
    "    num_layers: int = 6\n",
    "    dropout: float = 0.1\n",
    "    activation: str = \"gelu\"\n",
    "\n",
    "    # MLP hidden dimension is typically 4x the embed_dim in transformer architectures\n",
    "    mlp_hidden_dim: int = 1024\n",
    "\n",
    "    # Special token IDs (these should match the tokenizer's special tokens)\n",
    "    pad_token_id: int = 1\n",
    "    bos_token_id: int = 2\n",
    "    eos_token_id: int = 3\n",
    "\n"
   ],
   "id": "1c5b6b747599c5e4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:48.581079Z",
     "start_time": "2026-02-08T21:25:48.576938Z"
    }
   },
   "cell_type": "code",
   "source": "CFG = ModelConfig()",
   "id": "ca0ec5ed34e652df",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:48.586646Z",
     "start_time": "2026-02-08T21:25:48.581442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, mlp_hidden_dim: int, dropout: float, activation: str, *args: Any, **kwargs: Any):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.attention = MultiHeadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(\n",
    "            embed_dim = embed_dim,\n",
    "            hidden_dim = mlp_hidden_dim,\n",
    "            dropout = dropout,\n",
    "            activation = activation\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor | None = None, key_padding_mask: torch.Tensor | None = None, is_causal: bool = True) -> torch.Tensor:\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "        # Residual connection and layer normalization\n",
    "        x = x + self.attention(self.norm1(x), attn_mask=attn_mask, key_padding_mask=key_padding_mask, is_causal=is_causal)\n",
    "\n",
    "        # Residual connection and layer normalization\n",
    "        x = x +  self.mlp(self.norm2(x))\n",
    "\n",
    "\n",
    "        return x"
   ],
   "id": "301d6a1f2cdbf671",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Output layer and Transformer LM assembly",
   "id": "f4204d00502aa9af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:48.599545Z",
     "start_time": "2026-02-08T21:25:48.590247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, cfg: ModelConfig, *args: Any, **kwargs: Any):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.config = cfg\n",
    "        self.input_embedding = TransformerInputEmbedding(\n",
    "            vocab_size=cfg.vocab_size,\n",
    "            embed_dim=cfg.embed_dim,\n",
    "            max_seq_length=cfg.max_seq_length,\n",
    "        )\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                embed_dim=cfg.embed_dim,\n",
    "                num_heads=cfg.num_heads,\n",
    "                mlp_hidden_dim=cfg.mlp_hidden_dim,\n",
    "                dropout=cfg.dropout,\n",
    "                activation=cfg.activation\n",
    "            ) for _ in range(cfg.num_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(cfg.embed_dim, cfg.vocab_size)\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(cfg.embed_dim)\n",
    "        self.lm_head = nn.Linear(cfg.embed_dim, cfg.vocab_size, bias=False)\n",
    "\n",
    "        # Tie weights between input embedding and output layer\n",
    "        self.output_layer.weight = self.input_embedding.token_embedding.weight\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                is_causal: bool = True,\n",
    "                targets: torch.Tensor | None = None,\n",
    "                casual_mask: torch.Tensor | None = None,\n",
    "                key_padding_mask: torch.Tensor | None = None) -> tuple[torch.Tensor, torch.Tensor | None]:\n",
    "        # x: (batch, seq_len)\n",
    "        # targets: (batch, seq_len) or None\n",
    "\n",
    "        # if user didn't provide a padding mask, create one from pad_token_id\n",
    "        if key_padding_mask is None and self.config.pad_token_id is not None:\n",
    "            key_padding_mask = (x == self.config.pad_token_id)\n",
    "\n",
    "        x = self.input_embedding(x)  # (batch, seq_len, embed_dim)\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, attn_mask=casual_mask, key_padding_mask=key_padding_mask, is_causal=is_causal)\n",
    "\n",
    "        x = self.ln_f(x)  # (batch, seq_len, embed_dim)\n",
    "        logits = self.lm_head(x)  # (batch, seq_len, vocab_size\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Flatten the logits and targets for cross-entropy loss\n",
    "            B, T, V = logits.size()\n",
    "            logits_2d = logits.view(B * T, V)\n",
    "            targets_1d = targets.view(B * T)\n",
    "\n",
    "            # Ignore PAD in loss calculation\n",
    "            loss = F.cross_entropy(logits_2d, targets_1d, ignore_index=self.config.pad_token_id)\n",
    "\n",
    "        return logits, loss"
   ],
   "id": "4623f0c95c885ead",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train and evaluate the model\n",
    "\n",
    "#### Training loop with checkpointing and early stopping"
   ],
   "id": "e97425781242f9c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:25:48.612731Z",
     "start_time": "2026-02-08T21:25:48.599994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    checkpoint_path: str,\n",
    "    epoch: int,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    val_loss: float,\n",
    "):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"val_loss\": float(val_loss),\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model: nn.Module, checkpoint_path: str, device: str = \"cpu\") -> nn.Module:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(\n",
    "        f\"Loaded checkpoint from epoch {checkpoint['epoch']} \"\n",
    "        f\"with val_loss={checkpoint['val_loss']:.4f}\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def _unpack_batch(batch: Any) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n",
    "\n",
    "    key_padding_mask = None\n",
    "\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        if len(batch) == 2:\n",
    "            x, y = batch\n",
    "        elif len(batch) == 3:\n",
    "            x, y, key_padding_mask = batch\n",
    "        else:\n",
    "            raise ValueError(\"Batch tuple/list must be (x,y) or (x,y,mask).\")\n",
    "        return x, y, key_padding_mask\n",
    "\n",
    "    if isinstance(batch, dict):\n",
    "        # common naming variations\n",
    "        x = batch.get(\"input_ids\", batch.get(\"x\"))\n",
    "        y = batch.get(\"labels\", batch.get(\"y\"))\n",
    "        key_padding_mask = batch.get(\"key_padding_mask\", None)\n",
    "\n",
    "        if x is None or y is None:\n",
    "            raise ValueError(\"Batch dict must contain input_ids/x and labels/y.\")\n",
    "        return x, y, key_padding_mask\n",
    "\n",
    "    raise ValueError(\"Unsupported batch format.\")\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 1e-4,\n",
    "    checkpoint_dir: str = \"checkpoints\",\n",
    "    patience: int = 3,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    grad_clip: Optional[float] = 1.0,\n",
    ") -> Tuple[nn.Module, List[float], List[float]]:\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_dir = str(Path(checkpoint_dir))\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    bad_epochs = 0\n",
    "\n",
    "    train_losses: List[float] = []\n",
    "    val_losses: List[float] = []\n",
    "\n",
    "    best_path = os.path.join(checkpoint_dir, \"best_model.pt\")\n",
    "    last_path = os.path.join(checkpoint_dir, \"last_model.pt\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train the model for one epoch\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        n_train_batches = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x, y, key_padding_mask = _unpack_batch(batch)\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = key_padding_mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            _, loss = model(x, targets=y, key_padding_mask=key_padding_mask)\n",
    "            if loss is None:\n",
    "                raise RuntimeError(\"Model returned loss=None. Make sure you pass targets correctly.\")\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "            n_train_batches += 1\n",
    "\n",
    "        avg_train_loss = running_train_loss / max(1, n_train_batches)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        n_val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x, y, key_padding_mask = _unpack_batch(batch)\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                if key_padding_mask is not None:\n",
    "                    key_padding_mask = key_padding_mask.to(device)\n",
    "\n",
    "                _, loss = model(x, targets=y, key_padding_mask=key_padding_mask)\n",
    "                if loss is None:\n",
    "                    raise RuntimeError(\"Model returned loss=None during validation.\")\n",
    "\n",
    "                running_val_loss += loss.item()\n",
    "                n_val_batches += 1\n",
    "\n",
    "        avg_val_loss = running_val_loss / max(1, n_val_batches)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Save \"last\" checkpoint\n",
    "        save_checkpoint(\n",
    "            checkpoint_path=last_path,\n",
    "            epoch=epoch,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            val_loss=avg_val_loss,\n",
    "        )\n",
    "\n",
    "        # Check for improvement\n",
    "        improved = avg_val_loss < best_val_loss\n",
    "        if improved:\n",
    "            best_val_loss = avg_val_loss\n",
    "            bad_epochs = 0\n",
    "            save_checkpoint(\n",
    "                checkpoint_path=best_path,\n",
    "                epoch=epoch,\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                val_loss=avg_val_loss,\n",
    "            )\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d}/{num_epochs} | \"\n",
    "            f\"train_loss={avg_train_loss:.4f} | val_loss={avg_val_loss:.4f} | \"\n",
    "            f\"{'BEST' if improved else f'patience {bad_epochs}/{patience}'}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        if bad_epochs >= patience:\n",
    "            print(f\"Early stopping: no val improvement for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    # Load best weights back into the model before returning\n",
    "    model = load_checkpoint(model, best_path, device=device)\n",
    "    return model, train_losses, val_losses"
   ],
   "id": "2adbfea72513faa9",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the model",
   "id": "b906feb93b876d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:33:06.880947Z",
     "start_time": "2026-02-08T21:25:48.615027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "\n",
    "model = TransformerLM(CFG)\n",
    "model, train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=2,\n",
    "    learning_rate=3e-4,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    patience=5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")"
   ],
   "id": "9110ec6bd079ee63",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m CHECKPOINT_DIR = \u001B[33m\"\u001B[39m\u001B[33mcheckpoints\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      3\u001B[39m model = TransformerLM(CFG)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m model, train_losses, val_losses = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3e-4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcheckpoint_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCHECKPOINT_DIR\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpatience\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcuda\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_available\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcpu\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 100\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, val_loader, num_epochs, learning_rate, checkpoint_dir, patience, device, grad_clip)\u001B[39m\n\u001B[32m     96\u001B[39m     key_padding_mask = key_padding_mask.to(device)\n\u001B[32m     98\u001B[39m optimizer.zero_grad(set_to_none=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m _, loss = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m=\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    101\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    102\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mModel returned loss=None. Make sure you pass targets correctly.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Simple Transformer LLM Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Simple Transformer LLM Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 43\u001B[39m, in \u001B[36mTransformerLM.forward\u001B[39m\u001B[34m(self, x, is_causal, targets, casual_mask, key_padding_mask)\u001B[39m\n\u001B[32m     40\u001B[39m x = \u001B[38;5;28mself\u001B[39m.input_embedding(x)  \u001B[38;5;66;03m# (batch, seq_len, embed_dim)\u001B[39;00m\n\u001B[32m     42\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transformer_blocks:\n\u001B[32m---> \u001B[39m\u001B[32m43\u001B[39m     x = \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcasual_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     45\u001B[39m x = \u001B[38;5;28mself\u001B[39m.ln_f(x)  \u001B[38;5;66;03m# (batch, seq_len, embed_dim)\u001B[39;00m\n\u001B[32m     46\u001B[39m logits = \u001B[38;5;28mself\u001B[39m.lm_head(x)  \u001B[38;5;66;03m# (batch, seq_len, vocab_size\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Simple Transformer LLM Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Simple Transformer LLM Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 21\u001B[39m, in \u001B[36mTransformerBlock.forward\u001B[39m\u001B[34m(self, x, attn_mask, key_padding_mask, is_causal)\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch.Tensor, attn_mask: torch.Tensor | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m, key_padding_mask: torch.Tensor | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m, is_causal: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mTrue\u001B[39;00m) -> torch.Tensor:\n\u001B[32m     19\u001B[39m     \u001B[38;5;66;03m# x: (batch, seq_len, embed_dim)\u001B[39;00m\n\u001B[32m     20\u001B[39m     \u001B[38;5;66;03m# Residual connection and layer normalization\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m     x = x + \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnorm1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     23\u001B[39m     \u001B[38;5;66;03m# Residual connection and layer normalization\u001B[39;00m\n\u001B[32m     24\u001B[39m     x = x +  \u001B[38;5;28mself\u001B[39m.mlp(\u001B[38;5;28mself\u001B[39m.norm2(x))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Simple Transformer LLM Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Simple Transformer LLM Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 79\u001B[39m, in \u001B[36mMultiHeadAttention.forward\u001B[39m\u001B[34m(self, x, attn_mask, key_padding_mask, is_causal, return_attn)\u001B[39m\n\u001B[32m     77\u001B[39m \u001B[38;5;66;03m# Attention weights\u001B[39;00m\n\u001B[32m     78\u001B[39m attn_weights = F.softmax(attn_scores, dim=-\u001B[32m1\u001B[39m)  \u001B[38;5;66;03m# (batch, num_heads, seq_len, seq_len)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m79\u001B[39m attn_weights = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattn_weights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     81\u001B[39m \u001B[38;5;66;03m# Weighted sum of values\u001B[39;00m\n\u001B[32m     82\u001B[39m context = attn_weights @ v  \u001B[38;5;66;03m# (batch, num_heads, seq_len, head_dim)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Simple Transformer LLM Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Simple Transformer LLM Project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Simple Transformer LLM Project/.venv/lib/python3.13/site-packages/torch/nn/modules/dropout.py:73\u001B[39m, in \u001B[36mDropout.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m     70\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     71\u001B[39m \u001B[33;03m    Runs the forward pass.\u001B[39;00m\n\u001B[32m     72\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m73\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Simple Transformer LLM Project/.venv/lib/python3.13/site-packages/torch/nn/functional.py:1441\u001B[39m, in \u001B[36mdropout\u001B[39m\u001B[34m(input, p, training, inplace)\u001B[39m\n\u001B[32m   1438\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m p < \u001B[32m0.0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m p > \u001B[32m1.0\u001B[39m:\n\u001B[32m   1439\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdropout probability has to be between 0 and 1, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mp\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1440\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m-> \u001B[39m\u001B[32m1441\u001B[39m     _VF.dropout_(\u001B[38;5;28minput\u001B[39m, p, training) \u001B[38;5;28;01mif\u001B[39;00m inplace \u001B[38;5;28;01melse\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1442\u001B[39m )\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot training and validation loss curves",
   "id": "e1ed09214e5f3d55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_losses_detailed(train_losses: List[float], val_losses: List[float],\n",
    "                        smoothing_window: int = 5, save_path: str = None):\n",
    "    \"\"\"\n",
    "    Plot losses with optional smoothing for clearer trends.\n",
    "\n",
    "    Args:\n",
    "        train_losses: Training losses per epoch\n",
    "        val_losses: Validation losses per epoch\n",
    "        smoothing_window: Window size for moving average smoothing\n",
    "        save_path: Path to save the figure\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Raw losses\n",
    "    ax1.plot(epochs, train_losses, 'b-', alpha=0.6, label='Train Loss')\n",
    "    ax1.plot(epochs, val_losses, 'r-', alpha=0.6, label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Raw Losses')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Smoothed losses\n",
    "    if len(train_losses) > smoothing_window:\n",
    "        train_smooth = uniform_filter1d(train_losses, size=smoothing_window)\n",
    "        val_smooth = uniform_filter1d(val_losses, size=smoothing_window)\n",
    "\n",
    "        ax2.plot(epochs, train_smooth, 'b-', linewidth=2, label='Train Loss (smoothed)')\n",
    "        ax2.plot(epochs, val_smooth, 'r-', linewidth=2, label='Val Loss (smoothed)')\n",
    "    else:\n",
    "        ax2.plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "        ax2.plot(epochs, val_losses, 'r-', linewidth=2, label='Val Loss')\n",
    "\n",
    "    best_epoch = val_losses.index(min(val_losses)) + 1\n",
    "    ax2.axvline(x=best_epoch, color='g', linestyle='--', alpha=0.5)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title(f'Smoothed Losses (window={smoothing_window})')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    print(f\"\\nBest validation loss: {min(val_losses):.4f} at epoch {best_epoch}\")\n",
    "    print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"Final val loss: {val_losses[-1]:.4f}\")"
   ],
   "id": "1fd13e970e3a4641",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Iterative generation function",
   "id": "568ceb422b665a0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample_next_token(logits: torch.Tensor, temperature: float = 1.0, top_k: int | None = 50) -> int:\n",
    "\n",
    "    if temperature <= 0:\n",
    "        # Greedy\n",
    "        return int(torch.argmax(logits).item())\n",
    "\n",
    "    logits = logits / temperature\n",
    "\n",
    "    if top_k is not None and top_k > 0:\n",
    "        topk_vals, topk_idx = torch.topk(logits, k=min(top_k, logits.size(-1)))\n",
    "        probs = torch.softmax(topk_vals, dim=-1)\n",
    "        next_idx_in_topk = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        return int(topk_idx[next_idx_in_topk].item())\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    return int(torch.multinomial(probs, num_samples=1).item())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int | None = 50,\n",
    "    add_bos: bool = False,\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Encode prompt -> token ids\n",
    "    ids = tokenizer.encode(prompt).ids\n",
    "\n",
    "    if add_bos and hasattr(model, \"cfg\") and model.cfg.bos_token_id is not None:\n",
    "        ids = [model.cfg.bos_token_id] + ids\n",
    "\n",
    "    # Keep everything in a tensor: (1, T)\n",
    "    x = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # If context is longer than model max, crop from the left\n",
    "        if x.size(1) > model.cfg.max_seq_length:\n",
    "            x = x[:, -model.cfg.max_seq_length :]\n",
    "\n",
    "        logits, _ = model(x)              # logits: (1, T, vocab)\n",
    "        last_logits = logits[0, -1, :]    # (vocab_size,)\n",
    "\n",
    "        next_id = sample_next_token(last_logits, temperature=temperature, top_k=top_k)\n",
    "\n",
    "        # Append next token\n",
    "        next_token = torch.tensor([[next_id]], dtype=torch.long, device=device)\n",
    "        x = torch.cat([x, next_token], dim=1)\n",
    "\n",
    "        # Stop on EOS if you use it\n",
    "        if hasattr(model, \"cfg\") and model.cfg.eos_token_id is not None:\n",
    "            if next_id == model.cfg.eos_token_id:\n",
    "                break\n",
    "\n",
    "    # Decode full sequence\n",
    "    out_ids = x[0].tolist()\n",
    "    return tokenizer.decode(out_ids)"
   ],
   "id": "99b505593f2cc463",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate text with the trained model",
   "id": "7797a7bf49db1e0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load best model later\n",
    "best_model = TransformerLM(CFG)\n",
    "best_model = load_checkpoint(best_model, \"checkpoints/best_model.pt\", device=\"cuda\")\n",
    "\n",
    "prompt = \"To be, or not to be, that is the question:\"\n",
    "out = generate_text(\n",
    "    best_model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=40,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "print(out)"
   ],
   "id": "2cf0554b7f8217fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_losses_detailed(train_losses=train_losses, val_losses=val_losses, smoothing_window=3, save_path=\"loss_curves.png\")",
   "id": "1f47479def10a02c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
